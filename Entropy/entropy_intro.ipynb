{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, Compression, and Huffman Variable Length Encoding\n",
    "stough 202-\n",
    "\n",
    "Images and videos can be quite large. At 60 frames per second, a 4K Ultra HD movie of 2 hours could require \n",
    "\n",
    "\\begin{equation*}\n",
    "((3840\\cdot2160)pixels \\cdot 3\\frac{byte}{pixel} = 24883200 \\frac{byte}{frame}\\\\\n",
    "\\frac{24883200 \\frac{byte}{frame}\\cdot60\\frac{frame}{second}\\cdot60\\frac{second}{minute}\\cdot120 minutes}{10^9\\frac{byte}{GB}} = 10749.5 GB,\n",
    "\\end{equation*}\n",
    "\n",
    "or about 163 blu-ray discs to store without compression. So clearly compression is a thing. \n",
    "\n",
    "Compression takes advantage of different kinds of redundancy in the signal. Images in particular have a lot of redundancy. Spatial and Temporal redundancy are characterized by the fact that most pixels are closely related their neighbors. This is due to the fact that any images of human interest are likely to have objects comprised of regions of similar color. *Temporal* redundancy is the video case, where a pixel over time changes little usually. \n",
    "\n",
    "Coding redundancy is the one we're dealing with here. Most images we consider are 3-channel images with 8 bits per pixel per channel. We need 8 bits to represent a value in $[0, 255]$. Variable-length encoding can take advantage of the differential frequency of certain pixel values over others, encoding more common pixels with fewer bits, and paying that off by using more bits to represent less common pixels.\n",
    "\n",
    "- **Compressibility**: The degree to which the information in an image or video can be represented using fewer bits. In the example above of the highly compressible 4K Ultra HD, we might say the *compression ratio* is ~160 (display size over storage size). \n",
    "- **Entropy**: a measure of the information content in a signal. \n",
    "    - signal: here, think of this as seeing each individual pixel in the image in order.\n",
    "    - Think of entropy as the surprise, or \"unpredictability\" of the signal. Each new pixel value $k$ in the signal gives you some information,\n",
    "    \\begin{equation*}\n",
    "    I(k) = -\\log_2(p(k)),\n",
    "    \\end{equation*}\n",
    "    where $p(k)$ is the probability of observing $k$ in the signal. For example, if the probability of observing $k$ is 1, as in 100\\% of all the pixels have the value $k$, then there is no information/surpise, or 0 bits, associated with observing it. If $p(k) = \\frac{1}{2}$, then that's 1 bit of information. (We won't consider the symbols $\\kappa$ that *never* show up, since observing such a symbol would be infinitely surprising :-). \n",
    "    - For our images in the range $[0, 255]$, we can measure the entropy or information content of the image as \n",
    "    \\begin{equation*}\n",
    "    \\tilde{H} = -\\sum_{k=0}^{255} p(k)\\log_{2}p(k)\n",
    "    \\end{equation*}\n",
    "    This is just an average of how many bits of information each newly observed pixel gives us, weighted by how likely that pixel value is. $p(k)$ is just the normalized histogram values.\n",
    "    \n",
    "In this notebook we'll explore Huffman variable-length encoding its effectiveness in some images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Notice I've added our own custom Huffman utilities `huff_utils` and [`scipy.stats.entropy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html). Study `build_huff_tree` in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "import skimage.color as color\n",
    "from ipywidgets import VBox, HBox, FloatLogSlider\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# For importing from alternative directory sources\n",
    "import sys  \n",
    "sys.path.insert(0, '../dip_utils')\n",
    "\n",
    "\n",
    "from huff_utils import (build_huff_tree,\n",
    "                        build_huff_encoder,\n",
    "                        build_huff_pair,\n",
    "                        load_huffable_image,\n",
    "                        test_tree_making)\n",
    "from matrix_utils import (arr_info,\n",
    "                          make_linmap)\n",
    "from vis_utils import (vis_rgb_cube,\n",
    "                       vis_hsv_cube,\n",
    "                       vis_hists,\n",
    "                       vis_pair,\n",
    "                       lab_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Trees\n",
    "A Huffman tree is a binary tree structure whose leaf nodes represent the symbols in a signal (the pixel values in an image). The path from the root of the tree to a leaf node represents the bit encoding of the associated symbol (0 for left child, 1 for right child, for example). Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree_making()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we start with pixel values in ['A', 'B', 'C', 'D']. The Huffman Tree construction algorithm basically sorts these from least to most frequent (a heap is useful for this) as a list of leaf nodes. \n",
    "\n",
    "- We then take the two least frequent nodes and combine them into one Huffman tree with the two pixel values as leaf nodes and the frequency associated with this tree as the sum of the two individual frequencies. \n",
    "- Then insert this tree back into the list. This is where a heap comes in useful, as a self-maintaining structure always providing access to the least frequent node. \n",
    "- Repeat. Each iteration we end up with one fewer in the list, until there is only one tree left. \n",
    "\n",
    "The structure of that final tree represents an optimal encoding given the relative frequencies provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the entropy of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = plt.imread('../dip_pics/happy128.png')\n",
    "Ih = load_huffable_image(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ih = load_huffable_image('../dip_pics/happy128.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_info(Ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(Ih, cmap='gray', interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hists(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many bits would we need?\n",
    "8*np.prod(Ih.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(257)\n",
    "freq, bins = np.histogram(Ih.ravel(), bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the average actual information per pixel in the image?\n",
    "entropy(freq, base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build encoder and decoder dictionaries.\n",
    "See the code in `huff_utils`. One thing you'll notice about the encoder is that a lot of pixel values require many more than even 8 bits!!! How could that possibly save space?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = build_huff_pair(Ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode the image.\n",
    "enIh = ''.join([encoder[pix] for pix in Ih.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enIh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*np.prod(Ih.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression ratio: old bits to new bits needed.\n",
    "8*np.prod(Ih.shape)/len(enIh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bits per pixel we're actually using; compare to entropy determined above.\n",
    "len(enIh)/np.prod(Ih.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
