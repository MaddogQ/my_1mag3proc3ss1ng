{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using [torch](https://pytorch.org/docs/stable/cuda.html) to compare CPU/GPU speeds\n",
    "stough 202-\n",
    "\n",
    "The [Graphics Processing Unit](https://www.extremetech.com/gaming/269335-how-graphics-cards-work)\n",
    "is a common [coprocessor](https://en.wikipedia.org/wiki/Coprocessor) designed to do parallel floating point\n",
    "arithmetic. In the past this was computer graphics, but this massively parallel math is useful \n",
    "in all scientific computation.\n",
    "\n",
    "Also, going to use jupyterlab [magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1070'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce GTX 1070', major=6, minor=1, total_memory=8192MB, multi_processor_count=16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## We'll do a large matrix multiply operation\n",
    "in numpy, torch, and torch on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(400,1000,200)\n",
    "B = np.random.rand(400,200,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610.3515625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(8*A.size)/(1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.matmul(A,B)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3051.7578125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*C.size/(1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 s ± 152 ms per loop (mean ± std. dev. of 4 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 4\n",
    "# C = np.matmul(A,B)\n",
    "np.matmul(A,B, out=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Test in Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "At_cpu = torch.from_numpy(A)\n",
    "Bt_cpu = torch.from_numpy(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At_cpu.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 1000, 1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ct_cpu = torch.matmul(At_cpu, Bt_cpu)\n",
    "Ct_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963 ms ± 63.1 ms per loop (mean ± std. dev. of 4 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 4\n",
    "# Ct_cpu = torch.matmul(At_cpu, Bt_cpu)\n",
    "torch.matmul(At_cpu, Bt_cpu, out = Ct_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Now test in torch, on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "At_gpu = At_cpu.cuda()\n",
    "Bt_gpu = Bt_cpu.cuda()\n",
    "Ct_gpu = torch.zeros_like(Ct_cpu).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At_gpu.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 3273.01 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "25.9 ms ± 44.8 ms per loop (mean ± std. dev. of 4 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 4\n",
    "torch.matmul(At_gpu, Bt_gpu, out=Ct_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Try again, with our own timing.\n",
    "\n",
    "- [math expressions in markdown](https://stackoverflow.com/questions/48422762/is-it-possible-to-show-print-output-as-latex-in-jupyter-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "20 iters took 22315.03$\\mu$s per."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    torch.zeros(Ct_gpu.shape, out=Ct_gpu)\n",
    "    torch.matmul(At_gpu, Bt_gpu, out=Ct_gpu)\n",
    "    times.append(time.time() - st)\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "# Why not be more complicated...\n",
    "# print(f'20 iters took {1000000*(et-st):.2f}')\n",
    "display(Markdown(rf'20 iters took {1000000*(et-st):.2f}$\\mu$s per.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01884293556213379,\n",
       " 0.01884293556213379,\n",
       " 0.01884293556213379,\n",
       " 0.01884293556213379,\n",
       " 0.01884293556213379,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.01933884620666504,\n",
       " 0.019671201705932617,\n",
       " 0.019671201705932617,\n",
       " 0.019671201705932617,\n",
       " 0.019671201705932617,\n",
       " 0.019671201705932617,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.019834518432617188,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02033090591430664,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.02082657814025879,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021322965621948242,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695,\n",
       " 0.021819353103637695]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
