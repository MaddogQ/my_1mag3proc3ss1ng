{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using [torch](https://pytorch.org/docs/stable/cuda.html) to compare CPU/GPU speeds\n",
    "stough 202-\n",
    "\n",
    "The [Graphics Processing Unit](https://www.extremetech.com/gaming/269335-how-graphics-cards-work)\n",
    "is a common [coprocessor](https://en.wikipedia.org/wiki/Coprocessor) designed to do parallel floating point\n",
    "arithmetic. In the past this was computer graphics, but this massively parallel math is useful \n",
    "in all scientific computation.\n",
    "\n",
    "Also, going to use jupyterlab [magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce RTX 2080 Ti', major=7, minor=5, total_memory=11019MB, multi_processor_count=68)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## We'll do a large matrix multiply operation\n",
    "in numpy, torch, and torch on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(400,1000,200)\n",
    "B = np.random.rand(400,200,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610.3515625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(8*A.size)/(1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.matmul(A,B)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3051.7578125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*C.size/(1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 ms ± 631 µs per loop (mean ± std. dev. of 4 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 4\n",
    "# C = np.matmul(A,B)\n",
    "np.matmul(A,B, out=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Test in Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "At_cpu = torch.from_numpy(A)\n",
    "Bt_cpu = torch.from_numpy(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At_cpu.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 1000, 1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ct_cpu = torch.matmul(At_cpu, Bt_cpu)\n",
    "Ct_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447 ms ± 14.5 ms per loop (mean ± std. dev. of 4 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 4\n",
    "# Ct_cpu = torch.matmul(At_cpu, Bt_cpu)\n",
    "torch.matmul(At_cpu, Bt_cpu, out = Ct_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Now test in torch, on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "At_gpu = At_cpu.cuda()\n",
    "Bt_gpu = Bt_cpu.cuda()\n",
    "Ct_gpu = torch.zeros_like(Ct_cpu).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At_gpu.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10121.34 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "37.2 ms ± 64.4 ms per loop (mean ± std. dev. of 4 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 4\n",
    "torch.matmul(At_gpu, Bt_gpu, out=Ct_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Try again, with our own timing.\n",
    "\n",
    "- [math expressions in markdown](https://stackoverflow.com/questions/48422762/is-it-possible-to-show-print-output-as-latex-in-jupyter-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "20 iters took 6332.87$\\mu$s per."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    torch.zeros(Ct_gpu.shape, out=Ct_gpu)\n",
    "    torch.matmul(At_gpu, Bt_gpu, out=Ct_gpu)\n",
    "    times.append(time.time() - st)\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "# Why not be more complicated...\n",
    "# print(f'20 iters took {1000000*(et-st):.2f}')\n",
    "display(Markdown(rf'20 iters took {1000000*(et-st):.2f}$\\mu$s per.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0035178661346435547,\n",
       " 0.003555774688720703,\n",
       " 0.0035858154296875,\n",
       " 0.003614664077758789,\n",
       " 0.003643512725830078,\n",
       " 0.0036721229553222656,\n",
       " 0.00370025634765625,\n",
       " 0.003728151321411133,\n",
       " 0.0037565231323242188,\n",
       " 0.0037848949432373047,\n",
       " 0.0038127899169921875,\n",
       " 0.003840923309326172,\n",
       " 0.003869295120239258,\n",
       " 0.0038971900939941406,\n",
       " 0.0039250850677490234,\n",
       " 0.003952980041503906,\n",
       " 0.003981113433837891,\n",
       " 0.0040094852447509766,\n",
       " 0.004037618637084961,\n",
       " 0.004065752029418945,\n",
       " 0.004093647003173828,\n",
       " 0.004121541976928711,\n",
       " 0.004153728485107422,\n",
       " 0.004182100296020508,\n",
       " 0.004210233688354492,\n",
       " 0.004238128662109375,\n",
       " 0.004266262054443359,\n",
       " 0.004294395446777344,\n",
       " 0.004322052001953125,\n",
       " 0.004349946975708008,\n",
       " 0.004377841949462891,\n",
       " 0.004405498504638672,\n",
       " 0.004433631896972656,\n",
       " 0.0044612884521484375,\n",
       " 0.00448918342590332,\n",
       " 0.004517078399658203,\n",
       " 0.004547119140625,\n",
       " 0.004575014114379883,\n",
       " 0.004602670669555664,\n",
       " 0.004630327224731445,\n",
       " 0.00465846061706543,\n",
       " 0.004686117172241211,\n",
       " 0.004714012145996094,\n",
       " 0.0047419071197509766,\n",
       " 0.004769802093505859,\n",
       " 0.004797458648681641,\n",
       " 0.004825115203857422,\n",
       " 0.004853248596191406,\n",
       " 0.0048809051513671875,\n",
       " 0.00490880012512207,\n",
       " 0.0049364566802978516,\n",
       " 0.004964113235473633,\n",
       " 0.004991769790649414,\n",
       " 0.005019187927246094,\n",
       " 0.005047321319580078,\n",
       " 0.005074977874755859,\n",
       " 0.005102872848510742,\n",
       " 0.005130290985107422,\n",
       " 0.0051610469818115234,\n",
       " 0.005190372467041016,\n",
       " 0.005218505859375,\n",
       " 0.005246400833129883,\n",
       " 0.0052738189697265625,\n",
       " 0.005301475524902344,\n",
       " 0.005329132080078125,\n",
       " 0.005356788635253906,\n",
       " 0.0053844451904296875,\n",
       " 0.005412101745605469,\n",
       " 0.0054395198822021484,\n",
       " 0.005467414855957031,\n",
       " 0.005494594573974609,\n",
       " 0.005522251129150391,\n",
       " 0.005549907684326172,\n",
       " 0.005578279495239258,\n",
       " 0.005605936050415039,\n",
       " 0.005633831024169922,\n",
       " 0.005661725997924805,\n",
       " 0.005689382553100586,\n",
       " 0.005717039108276367,\n",
       " 0.0057446956634521484,\n",
       " 0.00577235221862793,\n",
       " 0.005800008773803711,\n",
       " 0.005827426910400391,\n",
       " 0.005855083465576172,\n",
       " 0.005882740020751953,\n",
       " 0.005910396575927734,\n",
       " 0.0059375762939453125,\n",
       " 0.005965232849121094,\n",
       " 0.0059926509857177734,\n",
       " 0.006020784378051758,\n",
       " 0.0060482025146484375,\n",
       " 0.006075859069824219,\n",
       " 0.006103515625,\n",
       " 0.00613093376159668,\n",
       " 0.006161212921142578,\n",
       " 0.006189107894897461,\n",
       " 0.006216764450073242,\n",
       " 0.0062444210052490234,\n",
       " 0.006271839141845703,\n",
       " 0.006299734115600586]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
